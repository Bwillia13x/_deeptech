"""Contract tests for Experiments (A/B Testing) UI.

Verifies that API responses match TypeScript types defined in the frontend.
Follows the pattern from test_contract_api_frontend.py and test_relationship_ui.py
"""

import pytest
from typing import Any, Dict, List

from signal_harvester.api import (
    Experiment,
    ExperimentRun,
    ExperimentComparison,
    Discovery,
    DiscoveryLabel,
)
from signal_harvester.experiment import (
    ExperimentConfig,
    ExperimentMetrics,
    create_experiment,
    list_experiments,
    get_experiment,
    get_experiment_runs,
    compare_experiments,
    add_discovery_label,
    get_labeled_artifacts,
)


# Note: We need to import or define these Pydantic models in api.py for proper validation
# For now we'll test the dict responses to ensure they match frontend types


class TestExperimentContract:
    """Contract tests for experiment types between API and frontend."""
    
    def test_experiment_response_structure(self, tmp_db_path: str):
        """Test that experiment response matches frontend types."""
        # Create a test experiment
        config = ExperimentConfig(
            scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25},
            source_filters=["arxiv", "github"],
            min_score_threshold=70.0,
            lookback_days=7,
            description="Test experiment for validation",
        )
        
        exp_id = create_experiment(
            db_path=tmp_db_path,
            name="Test Experiment",
            config=config,
            baseline_id=None
        )
        
        # Get experiment
        response = get_experiment(tmp_db_path, exp_id)
        assert response is not None, "Experiment should exist"
        
        # Verify structure
        assert "id" in response
        assert "name" in response
        assert "description" in response
        assert "config" in response
        assert "baselineId" in response
        assert "status" in response
        assert "createdAt" in response
        assert "updatedAt" in response
        
        # Verify types
        assert isinstance(response["id"], int)
        assert isinstance(response["name"], str)
        assert isinstance(response["config"], dict)
        assert isinstance(response["status"], str)
        assert isinstance(response["createdAt"], str)
        assert isinstance(response["updatedAt"], str)
        
        # Verify config structure
        config_dict = response["config"]
        assert "scoringWeights" in config_dict
        assert isinstance(config_dict["scoringWeights"], dict)
        assert "sourceFilters" in config_dict
        assert isinstance(config_dict["sourceFilters"], list)
        assert "minScoreThreshold" in config_dict
        assert isinstance(config_dict["minScoreThreshold"], (int, float))
        assert "lookbackDays" in config_dict
        assert isinstance(config_dict["lookbackDays"], int)
        
        # Verify status is valid
        valid_statuses = {"draft", "running", "completed", "failed"}
        assert response["status"] in valid_statuses
    
    def test_experiment_list_response_structure(self, tmp_db_path: str):
        """Test that experiment list response matches frontend types."""
        # Create multiple experiments
        for i in range(3):
            config = ExperimentConfig(
                scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25},
                description=f"Test experiment {i}",
            )
            create_experiment(tmp_db_path, f"Test {i}", config)
        
        # Get list
        experiments = list_experiments(tmp_db_path)
        
        assert isinstance(experiments, list)
        assert len(experiments) >= 3
        
        # Verify first experiment structure
        exp = experiments[0]
        assert "id" in exp
        assert "name" in exp
        assert "config" in exp
        assert "status" in exp
        
        # Verify camelCase consistency (frontend uses camelCase)
        assert "baselineId" in exp or "baseline_id" in exp
    
    def test_experiment_run_response_structure(self, tmp_db_path: str):
        """Test that experiment run response matches frontend types."""
        # Create experiment and run
        config = ExperimentConfig(
            scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25},
        )
        exp_id = create_experiment(tmp_db_path, "Run Test", config)
        
        # Calculate metrics
        metrics = ExperimentMetrics(
            artifact_count=100,
            true_positives=80,
            false_positives=10,
            true_negatives=5,
            false_negatives=5,
            precision=0.8,
            recall=0.8,
            f1_score=0.8,
            accuracy=0.85,
        )
        
        # Record run
        from signal_harvester.experiment import create_experiment_run
        run_id = create_experiment_run(tmp_db_path, exp_id, metrics)
        
        # Get runs
        runs = get_experiment_runs(tmp_db_path, exp_id)
        
        assert isinstance(runs, list)
        assert len(runs) > 0
        
        run = runs[0]
        assert "id" in run
        assert "experiment_id" in run
        assert "artifact_count" in run
        assert "true_positives" in run
        assert "false_positives" in run
        assert "true_negatives" in run
        assert "false_negatives" in run
        assert "precision" in run
        assert "recall" in run
        assert "f1_score" in run or "f1Score" in run
        assert "accuracy" in run
        assert "started_at" in run or "startedAt" in run
        assert "completed_at" in run or "completedAt" in run
        assert "status" in run
        
        # Verify metrics are floats
        assert isinstance(run.get("precision", 0), (int, float))
        assert 0.0 <= run["precision"] <= 1.0
    
    def test_experiment_comparison_response_structure(self, tmp_db_path: str):
        """Test that experiment comparison response matches frontend types."""
        # Create two experiments
        config1 = ExperimentConfig(scoring_weights={"novelty": 0.3, "emergence": 0.3, "obscurity": 0.2, "confidence": 0.2})
        config2 = ExperimentConfig(scoring_weights={"novelty": 0.2, "emergence": 0.2, "obscurity": 0.3, "confidence": 0.3})
        
        exp1_id = create_experiment(tmp_db_path, "Baseline", config1)
        exp2_id = create_experiment(tmp_db_path, "Variant", config2, baseline_id=exp1_id)
        
        # Add runs for both
        metrics1 = ExperimentMetrics(
            artifact_count=100,
            true_positives=75,
            false_positives=10,
            true_negatives=10,
            false_negatives=5,
            precision=0.75,
            recall=0.75,
            f1_score=0.75,
            accuracy=0.85,
        )
        metrics2 = ExperimentMetrics(
            artifact_count=100,
            true_positives=80,
            false_positives=8,
            true_negatives=7,
            false_negatives=5,
            precision=0.80,
            recall=0.80,
            f1_score=0.80,
            accuracy=0.87,
        )
        
        from signal_harvester.experiment import create_experiment_run
        create_experiment_run(tmp_db_path, exp1_id, metrics1)
        create_experiment_run(tmp_db_path, exp2_id, metrics2)
        
        # Compare
        comparison = compare_experiments(tmp_db_path, exp1_id, exp2_id)
        
        assert isinstance(comparison, dict)
        assert "experimentA" in comparison
        assert "experimentB" in comparison
        assert "deltas" in comparison
        assert "winner" in comparison
        
        # Verify experiment structures
        exp_a = comparison["experimentA"]
        assert "id" in exp_a
        assert "precision" in exp_a
        assert "recall" in exp_a
        assert "f1Score" in exp_a or "f1_score" in exp_a
        assert "accuracy" in exp_a
        assert "artifactCount" in exp_a or "artifact_count" in exp_a
        
        # Verify deltas
        deltas = comparison["deltas"]
        assert "precision" in deltas
        assert "recall" in deltas
        assert "f1Score" in deltas or "f1_score" in deltas
        assert "accuracy" in deltas
        
        # Verify winner
        assert comparison["winner"] in {"experimentA", "experimentB", "tie"}
    
    def test_discovery_label_response_structure(self, tmp_db_path: str):
        """Test that discovery label response matches frontend types."""
        # Add a label
        artifact_id = 1  # Assuming artifact 1 exists
        label_id = add_discovery_label(
            db_path=tmp_db_path,
            artifact_id=artifact_id,
            label="true_positive",
            confidence=0.95,
            annotator="test-user"
        )
        
        # Get labels
        labels = get_labeled_artifacts(tmp_db_path)
        
        assert isinstance(labels, list)
        assert len(labels) > 0
        
        label = labels[0]
        assert "id" in label
        assert "artifactId" in label or "artifact_id" in label
        assert "label" in label
        assert "confidence" in label
        assert "annotator" in label
        assert "createdAt" in label or "created_at" in label
        assert "updatedAt" in label or "updated_at" in label
        
        # Verify types
        assert isinstance(label["label"], str)
        assert isinstance(label["confidence"], (int, float))
        assert 0.0 <= label["confidence"] <= 1.0
        
        # Verify valid label values
        valid_labels = {
            "true_positive", "false_positive", "true_negative", "false_negative",
            "relevant", "irrelevant"
        }
        assert label["label"] in valid_labels


class TestExperimentUIScenarios:
    """Test scenarios that verify the full experiment UI flow."""
    
    def test_experiment_creation_workflow(self, tmp_db_path: str):
        """Test the complete experiment creation workflow."""
        # 1. Create experiment
        config = ExperimentConfig(
            scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25},
            source_filters=["arxiv", "github"],
            min_score_threshold=70.0,
            lookback_days=7,
            description="Test workflow",
        )
        exp_id = create_experiment(tmp_db_path, "Workflow Test", config)
        
        # 2. Verify created
        exp = get_experiment(tmp_db_path, exp_id)
        assert exp is not None
        assert exp["name"] == "Workflow Test"
        assert exp["status"] == "draft"
        
        # 3. Verify in list
        experiments = list_experiments(tmp_db_path)
        assert any(e["id"] == exp_id for e in experiments)
        
        # 4. Verify structure matches frontend expectations
        assert "config" in exp
        assert "scoringWeights" in exp["config"]
        assert "sourceFilters" in exp["config"]
    
    def test_experiment_run_workflow(self, tmp_db_path: str):
        """Test the complete experiment run workflow."""
        # 1. Create experiment
        config = ExperimentConfig(
            scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25},
        )
        exp_id = create_experiment(tmp_db_path, "Run Test", config)
        
        # 2. Calculate metrics (simulating run)
        metrics = ExperimentMetrics(
            artifact_count=150,
            true_positives=100,
            false_positives=20,
            true_negatives=20,
            false_negatives=10,
            precision=0.833,
            recall=0.909,
            f1_score=0.869,
            accuracy=0.867,
        )
        
        # 3. Record run
        from signal_harvester.experiment import create_experiment_run
        run_id = create_experiment_run(tmp_db_path, exp_id, metrics, metadata={"scoring_model": "v2"})
        assert run_id > 0
        
        # 4. Get runs
        runs = get_experiment_runs(tmp_db_path, exp_id)
        assert len(runs) > 0
        assert runs[0]["experiment_id"] == exp_id
        assert runs[0]["metadata"] is None or isinstance(runs[0]["metadata"], dict)
        
        # 5. Verify structure
        run = runs[0]
        assert "precision" in run
        assert "recall" in run
        assert "f1_score" in run or "f1Score" in run
        assert "accuracy" in run
    
    def test_label_workflow(self, tmp_db_path: str):
        """Test the complete label annotation workflow."""
        # 1. Add label
        label_id = add_discovery_label(
            db_path=tmp_db_path,
            artifact_id=1,
            label="true_positive",
            confidence=0.9,
            annotator="test-user-1"
        )
        assert label_id > 0
        
        # 2. Add another label
        label_id2 = add_discovery_label(
            db_path=tmp_db_path,
            artifact_id=2,
            label="false_positive",
            confidence=0.85,
            annotator="test-user-2"
        )
        assert label_id2 > label_id
        
        # 3. Update existing label
        updated_id = add_discovery_label(
            db_path=tmp_db_path,
            artifact_id=1,
            label="true_positive",
            confidence=0.95,
            annotator="test-user-3"
        )
        assert updated_id == label_id  # Should update, not create new
        
        # 4. Get all labels
        labels = get_labeled_artifacts(tmp_db_path)
        assert len(labels) >= 2
        
        # 5. Filter by label
        tp_labels = get_labeled_artifacts(tmp_db_path, "true_positive")
        assert len(tp_labels) >= 1
        assert all(l["label"] == "true_positive" for l in tp_labels)


class TestExperimentConfigValidation:
    """Test experiment configuration validation."""
    
    def test_scoring_weights_normalization(self):
        """Test that scoring weights sum to 1.0."""
        config = ExperimentConfig(
            scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25}
        )
        
        total = sum(config.scoring_weights.values())
        assert total == 1.0
    
    def test_source_filter_validation(self):
        """Test source filter validation."""
        valid_sources = ["arxiv", "github", "x", "crossref", "semantic"]
        
        config = ExperimentConfig(
            scoring_weights={"novelty": 0.25, "emergence": 0.25, "obscurity": 0.25, "confidence": 0.25},
            source_filters=valid_sources
        )
        
        assert len(config.source_filters) == 5
        assert all(s in valid_sources for s in config.source_filters)


def test_metrics_accuracy_calculation():
    """Test that metrics calculation is accurate."""
    # Perfect prediction
    metrics = ExperimentMetrics(
        artifact_count=100,
        true_positives=50,
        false_positives=0,
        true_negatives=50,
        false_negatives=0,
        precision=1.0,
        recall=1.0,
        f1_score=1.0,
        accuracy=1.0,
    )
    
    assert metrics.accuracy == 1.0
    assert metrics.precision == 1.0
    assert metrics.recall == 1.0
    assert metrics.f1_score == 1.0
    
    # No positives case
    metrics2 = ExperimentMetrics(
        artifact_count=100,
        true_positives=0,
        false_positives=0,
        true_negatives=100,
        false_negatives=0,
        precision=0.0,  # Should be 0 when no positives
        recall=0.0,
        f1_score=0.0,
        accuracy=1.0,
    )
    
    assert metrics2.precision == 0.0
    assert metrics2.recall == 0.0
    # F1 should be 0 when precision or recall is 0
    assert metrics2.f1_score == 0.0
    assert metrics2.accuracy == 1.0


# Add these Pydantic models to api.py if not already present
# They should match the TypeScript types for contract testing

class ExperimentModel(BaseModel):
    """Pydantic model for Experiment to ensure API-frontend contract."""
    id: int
    name: str
    description: Optional[str]
    config: Dict[str, Any]
    baselineId: Optional[int]
    status: str
    createdAt: str
    updatedAt: str


class ExperimentRunModel(BaseModel):
    """Pydantic model for ExperimentRun to ensure API-frontend contract."""
    id: int
    experimentId: int
    artifactCount: int
    truePositives: int
    falsePositives: int
    trueNegatives: int
    falseNegatives: int
    precision: float
    recall: float
    f1Score: float
    accuracy: float
    startedAt: str
    completedAt: str
    status: str
    metadata: Optional[Dict[str, Any]]


class ExperimentComparisonModel(BaseModel):
    """Pydantic model for ExperimentComparison to ensure API-frontend contract."""
    experimentA: Dict[str, Any]
    experimentB: Dict[str, Any]
    deltas: Dict[str, float]
    winner: str


class TestModelValidation:
    """Test that Pydantic models validate API responses correctly."""
    
    def test_experiment_model_validation(self):
        """Test Experiment Pydantic model validation."""
        experiment_data = {
            "id": 1,
            "name": "Test Experiment",
            "description": "A test experiment",
            "config": {
                "scoringWeights": {"novelty": 0.25, "emergence": 0.25},
                "sourceFilters": ["arxiv"],
                "minScoreThreshold": 70.0,
                "lookbackDays": 7,
            },
            "baselineId": None,
            "status": "draft",
            "createdAt": "2025-01-01T00:00:00Z",
            "updatedAt": "2025-01-01T00:00:00Z",
        }
        
        model = ExperimentModel(**experiment_data)
        assert model.name == "Test Experiment"
        assert model.status == "draft"
    
    def test_experiment_run_model_validation(self):
        """Test ExperimentRun Pydantic model validation."""
        run_data = {
            "id": 1,
            "experimentId": 1,
            "artifactCount": 100,
            "truePositives": 80,
            "falsePositives": 10,
            "trueNegatives": 8,
            "falseNegatives": 2,
            "precision": 0.8,
            "recall": 0.8,
            "f1Score": 0.8,
            "accuracy": 0.88,
            "startedAt": "2025-01-01T00:00:00Z",
            "completedAt": "2025-01-01T01:00:00Z",
            "status": "completed",
        }
        
        model = ExperimentRunModel(**run_data)
        assert model.precision == 0.8
        assert model.f1Score == 0.8
